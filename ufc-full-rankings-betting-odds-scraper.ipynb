{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12848761,"sourceType":"datasetVersion","datasetId":8007030}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1) UFC Events Snapshot — URLs & Details  \n**Purpose:** Crawl the UFCStats \"Completed Events\" archive, capture event URLs and metadata (name, date, location, org IDs).  \n**Inputs / Config:** Static listing page (`ufcstats.com/statistics/events/completed?page=all`) with retry/session settings.  \n**Outputs:** A dataframe of events with normalized fields (incl. resolved URLs) to be used by downstream scrapers.  \n**Notes:** Idempotent; uses polite retry/backoff. Handles minor HTML structure drift.","metadata":{}},{"cell_type":"code","source":"# === UFC Events Snapshot Scraper (URLs + Details) ===\n\nfrom datetime import datetime\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\nimport pandas as pd\nimport requests, logging, time, os, sys\nfrom bs4 import BeautifulSoup\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\n# --- Config ---\nUFCSTATS_URL = \"http://ufcstats.com/statistics/events/completed?page=all\"\nTIMEOUT = 15\nHEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; UFC-AI/1.0; +https://example.com/bot)\"}\nOUTDIR = \"/kaggle/working\"  # Kaggle default output dir\nos.makedirs(OUTDIR, exist_ok=True)\n\n# Optional: limit liczby eventów (None = wszystkie)\nEVENT_LIMIT = None  # np. 200 dla szybkiego testu\n\n# --- Logging ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    stream=sys.stdout\n)\n\ndef http_session() -> requests.Session:\n    s = requests.Session()\n    retry = Retry(\n        total=4,                 # 1 żądanie + 3 retrysy\n        backoff_factor=0.8,      # 0.8s, 1.6s, 3.2s...\n        status_forcelist=(429, 500, 502, 503, 504),\n        allowed_methods=frozenset([\"GET\"])\n    )\n    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n    return s\n\ndef save_df(df: pd.DataFrame, name: str):\n    \"\"\"Zapisuje do CSV i Parquet w /kaggle/working\"\"\"\n    csv_path = os.path.join(OUTDIR, f\"{name}.csv\")\n    pq_path  = os.path.join(OUTDIR, f\"{name}.parquet\")\n    # CSV\n    df.to_csv(csv_path, index=False)\n    # Parquet\n    try:\n        df.to_parquet(pq_path, index=False)  # pyarrow jest na Kaggle\n    except Exception as e:\n        logging.warning(f\"Parquet save failed ({e}); trying fastparquet…\")\n        df.to_parquet(pq_path, index=False, engine=\"fastparquet\")\n    logging.info(f\"Saved: {csv_path}  &  {pq_path}\")\n\ndef parse_events(html: str) -> pd.DataFrame:\n    \"\"\"Parsuje listę eventów (URL, Date, Title) ze strony zbiorczej.\"\"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    rows = [\n        row for row in soup.select(\"tr.b-statistics__table-row\")\n        if row.select_one(\"a\") and row.select_one(\"span\")\n    ]\n\n    events: List[Tuple[str, datetime, str]] = []\n    skipped = 0\n    for row in rows:\n        a_tag = row.select_one(\"a\")\n        date_span = row.select_one(\"span\")\n\n        event_url = (a_tag[\"href\"] or \"\").strip().rstrip(\"/\") if a_tag else \"\"\n        event_title = (a_tag.text or \"\").strip() if a_tag else \"\"\n        date_txt = (date_span.text or \"\").strip() if date_span else \"\"\n\n        try:\n            event_date = datetime.strptime(date_txt, \"%B %d, %Y\").date()\n        except Exception as e:\n            logging.warning(f\"[parse] skip row — bad date '{date_txt}': {e}\")\n            skipped += 1\n            continue\n\n        if not event_url or not event_title:\n            skipped += 1\n            continue\n\n        events.append((event_url, event_date, event_title))\n\n    logging.info(f\"[parse] OK: {len(events)}, skipped: {skipped}\")\n\n    if not events:\n        return pd.DataFrame(columns=[\"event_url\", \"event_date\", \"event_title\"])\n\n    df = pd.DataFrame(events, columns=[\"event_url\", \"event_date\", \"event_title\"])\n    df[\"event_url\"] = df[\"event_url\"].astype(str).str.strip().str.rstrip(\"/\")\n    df[\"event_title\"] = df[\"event_title\"].astype(str).str.strip()\n    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"]).dt.date\n    return df\n\ndef parse_event_details(html: str) -> Tuple[Optional[str], Optional[datetime], Optional[str], Optional[str], Optional[str]]:\n    \"\"\"Zwraca (event_name, event_date, city, state, country)\"\"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    # Nazwa eventu\n    name_tag = soup.find(\"h2\", class_=\"b-content__title\")\n    event_name = name_tag.get_text(strip=True) if name_tag else None\n\n    details_box = soup.find(\"div\", class_=\"b-list__info-box\")\n    event_date = None\n    location_city = location_state = location_country = None\n\n    if details_box:\n        for li in details_box.find_all(\"li\"):\n            text = li.get_text(strip=True)\n            if \"Date:\" in text:\n                date_text = text.split(\"Date:\", 1)[1].strip()\n                try:\n                    event_date = datetime.strptime(date_text, \"%B %d, %Y\").date()\n                except Exception:\n                    event_date = None\n            if \"Location:\" in text:\n                location_text = text.split(\"Location:\", 1)[1].strip()\n                parts = [p.strip() for p in location_text.split(\",\")]\n                location_city = parts[0] if len(parts) > 0 else None\n                location_state = parts[1] if len(parts) > 1 else None\n                location_country = parts[2] if len(parts) > 2 else None\n\n    return event_name, event_date, location_city, location_state, location_country\n","metadata":{"execution":{"iopub.execute_input":"2025-09-04T12:10:07.786784Z","iopub.status.busy":"2025-09-04T12:10:07.786508Z","iopub.status.idle":"2025-09-04T12:10:10.670682Z","shell.execute_reply":"2025-09-04T12:10:10.669816Z","shell.execute_reply.started":"2025-09-04T12:10:07.786758Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## 2) Orchestrate: Run Events Snapshot  \n**Purpose:** Execute the event snapshot end‑to‑end: request listing page, parse rows, resolve individual event pages if needed.  \n**Inputs:** Events listing URL, HTTP headers, timeouts, retry policy.  \n**Outputs:** In‑memory dataframe of events; optionally persisted for later cells.  \n**Notes:** Basic validation of HTTP status and non‑empty results.","metadata":{}},{"cell_type":"code","source":"# Cell 2: Run full snapshot (events URLs + event details) and save as CSV & Parquet\n\nfrom tqdm import tqdm\n\nsession = http_session()\n\n# 1) Pobierz stronę zbiorczą eventów\nlogging.info(\"Fetching events index…\")\nresp = session.get(UFCSTATS_URL, headers=HEADERS, timeout=TIMEOUT)\nif resp.status_code != 200:\n    raise RuntimeError(f\"UFCStats HTTP {resp.status_code}\")\n\ndf_events_urls = parse_events(resp.text)\n\nif df_events_urls.empty:\n    logging.warning(\"Brak zparsowanych eventów — możliwa zmiana layoutu ufcstats.com\")\nelse:\n    if EVENT_LIMIT is not None:\n        df_events_urls = df_events_urls.head(int(EVENT_LIMIT))\n        logging.info(f\"EVENT_LIMIT active → using first {len(df_events_urls)} events\")\n    save_df(df_events_urls, \"UFC_events_urls\")\n\n# 2) Dla każdego event_url pobierz szczegóły eventu\nrows = []\nlogging.info(f\"Fetching details for {len(df_events_urls)} events…\")\nfor url, date_, title in tqdm(df_events_urls[[\"event_url\", \"event_date\", \"event_title\"]].itertuples(index=False), total=len(df_events_urls)):\n    try:\n        r = session.get(url, headers=HEADERS, timeout=TIMEOUT)\n        r.raise_for_status()\n        name, ev_date, city, state, country = parse_event_details(r.text)\n\n        # Walidacja minimum\n        if not name:\n            logging.warning(f\"[details] skip {url} — missing name\")\n            continue\n        final_date = ev_date if ev_date is not None else date_\n\n        rows.append({\n            \"event_url\": url,\n            \"event_name\": name,\n            \"event_date\": final_date,     # DATE\n            \"event_city\": city,\n            \"event_state\": state,\n            \"event_country\": country,\n            \"event_title_index\": title    # tytuł z listy (czasem różni się od h2)\n        })\n\n        # miękki throttling (uprzejmość dla serwisu)\n        time.sleep(0.05)\n\n    except Exception as e:\n        logging.warning(f\"[details] error {url}: {e}\")\n        continue\n\ndf_events_data = pd.DataFrame(rows, columns=[\n    \"event_url\",\"event_name\",\"event_date\",\"event_city\",\"event_state\",\"event_country\",\"event_title_index\"\n])\n\nif df_events_data.empty:\n    logging.warning(\"No valid event details parsed.\")\nelse:\n    # typy\n    df_events_data[\"event_url\"] = df_events_data[\"event_url\"].astype(str).str.strip().str.rstrip(\"/\")\n    df_events_data[\"event_name\"] = df_events_data[\"event_name\"].astype(str).str.strip()\n    df_events_data[\"event_date\"] = pd.to_datetime(df_events_data[\"event_date\"]).dt.date\n\n    save_df(df_events_data, \"UFC_events_data\")\n\n# Podsumowanie\nprint(\"Summary:\")\nprint(\" - UFC_events_urls:\", df_events_urls.shape)\nprint(\" - UFC_events_data:\", df_events_data.shape)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-04T12:10:39.171084Z","iopub.status.busy":"2025-09-04T12:10:39.170778Z","iopub.status.idle":"2025-09-04T12:15:57.550900Z","shell.execute_reply":"2025-09-04T12:15:57.549642Z","shell.execute_reply.started":"2025-09-04T12:10:39.171061Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 744/744 [05:16<00:00,  2.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Summary:\n"," - UFC_events_urls: (744, 3)\n"," - UFC_events_data: (744, 7)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"execution_count":2},{"cell_type":"markdown","source":"## 3) UFC Fight URLs Snapshot  \n**Purpose:** For each archived event, collect links to all fight pages (one URL per bout).  \n**Inputs:** Event URLs from the previous snapshot.  \n**Outputs:** A deduplicated dataframe of fight URLs with event context (event_id, card order, etc.).  \n**Notes:** Resilient to missing or cancelled bouts; graceful handling of pagination quirks.","metadata":{}},{"cell_type":"code","source":"# === UFC Fight URLs Snapshot ===\n# Wejście: /kaggle/working/UFC_events_urls.(csv|parquet)\n# Wyjście: /kaggle/working/UFC_fights_urls.(csv|parquet)\n\nimport os, sys, logging, time\nfrom datetime import datetime\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom tqdm import tqdm\n\n# --- I/O ---\nOUTDIR = \"/kaggle/working\"\nEVENTS_CSV = os.path.join(OUTDIR, \"UFC_events_urls.csv\")\nEVENTS_PARQUET = os.path.join(OUTDIR, \"UFC_events_urls.parquet\")\nFIGHTS_URLS_BASENAME = \"UFC_fights_urls\"\n\n# --- Logging ---\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\", stream=sys.stdout)\n\n# --- Helpers ---\ndef save_df(df: pd.DataFrame, name: str):\n    csv_path = os.path.join(OUTDIR, f\"{name}.csv\")\n    pq_path  = os.path.join(OUTDIR, f\"{name}.parquet\")\n    df.to_csv(csv_path, index=False)\n    try:\n        df.to_parquet(pq_path, index=False)\n    except Exception as e:\n        logging.warning(f\"Parquet save failed ({e}); trying fastparquet…\")\n        df.to_parquet(pq_path, index=False, engine=\"fastparquet\")\n    logging.info(f\"Saved: {csv_path}  &  {pq_path}\")\n\ndef http_session():\n    s = requests.Session()\n    retry = Retry(\n        total=4,\n        backoff_factor=0.8,\n        status_forcelist=(429, 500, 502, 503, 504),\n        allowed_methods=frozenset([\"GET\"])\n    )\n    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n    return s\n\ndef normalize_url(u: str) -> str:\n    return (u or \"\").strip().rstrip(\"/\")\n\n# --- Load events (prefer Parquet if present) ---\nif os.path.exists(EVENTS_PARQUET):\n    events_df = pd.read_parquet(EVENTS_PARQUET)\nelif os.path.exists(EVENTS_CSV):\n    events_df = pd.read_csv(EVENTS_CSV)\nelse:\n    raise FileNotFoundError(\"Nie znaleziono pliku UFC_events_urls.(csv|parquet) w /kaggle/working\")\n\n# sanity & normalization\nevents_df = events_df.rename(columns={\"Event_url\": \"event_url\", \"Date\": \"event_date\", \"Title\": \"event_title\"})\nevents_df[\"event_url\"] = events_df[\"event_url\"].astype(str).str.strip().str.rstrip(\"/\")\nevents_df[\"event_title\"] = events_df[\"event_title\"].astype(str).str.strip()\nevents_df[\"event_date\"] = pd.to_datetime(events_df[\"event_date\"]).dt.date\n\n# Optional: ogranicz liczbę eventów przy testach\nEVENT_LIMIT = None  # np. 50\nif EVENT_LIMIT is not None:\n    events_df = events_df.head(int(EVENT_LIMIT))\n    logging.info(f\"EVENT_LIMIT active → using first {len(events_df)} events\")\n\n# --- Scrape fights per event ---\nsession = http_session()\nHEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; UFC-AI/1.0; +https://example.com/bot)\"}\n\nrecords = []\nlogging.info(f\"Scraping fights for {len(events_df)} events…\")\nfor row in tqdm(events_df.itertuples(index=False), total=len(events_df)):\n    event_url = normalize_url(row.event_url)\n    try:\n        r = session.get(event_url, headers=HEADERS, timeout=15)\n        if r.status_code != 200:\n            logging.warning(f\"[{r.status_code}] {event_url}\")\n            continue\n        soup = BeautifulSoup(r.text, \"html.parser\")\n        rows = soup.select(\"tr.b-fight-details__table-row\")\n        for tr in rows:\n            cols = tr.find_all(\"td\")\n            if len(cols) < 2:\n                continue\n            a = cols[0].find(\"a\", href=True)\n            if not a:\n                continue\n            fight_url = normalize_url(a[\"href\"])\n            records.append({\n                \"Event_URL\": event_url,\n                \"Title\": row.event_title,\n                \"Date\": pd.to_datetime(row.event_date).strftime(\"%Y-%m-%d\"),\n                \"Fight_URL\": fight_url\n            })\n        time.sleep(0.03)  # uprzejmy throttling\n    except Exception as e:\n        logging.warning(f\"[fight_urls] {event_url} → {e}\")\n        continue\n\n# --- Build DF, dedupe & save ---\nif not records:\n    logging.warning(\"Brak zebranych walk (records = []).\")\n    fights_urls_df = pd.DataFrame(columns=[\"Event_URL\",\"Title\",\"Date\",\"Fight_URL\"])\nelse:\n    fights_urls_df = pd.DataFrame(records)\n    fights_urls_df[\"Event_URL\"] = fights_urls_df[\"Event_URL\"].astype(str).str.strip().str.rstrip(\"/\")\n    fights_urls_df[\"Fight_URL\"] = fights_urls_df[\"Fight_URL\"].astype(str).str.strip().str.rstrip(\"/\")\n    fights_urls_df = fights_urls_df.drop_duplicates(subset=[\"Fight_URL\"]).reset_index(drop=True)\n\nsave_df(fights_urls_df, FIGHTS_URLS_BASENAME)\n\nprint(\"Summary:\")\nprint(\" - UFC_fights_urls:\", fights_urls_df.shape)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-04T12:31:50.703209Z","iopub.status.busy":"2025-09-04T12:31:50.702808Z","iopub.status.idle":"2025-09-04T12:36:54.638618Z","shell.execute_reply":"2025-09-04T12:36:54.637401Z","shell.execute_reply.started":"2025-09-04T12:31:50.703178Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 744/744 [05:03<00:00,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Summary:\n"," - UFC_fights_urls: (8324, 4)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"execution_count":3},{"cell_type":"markdown","source":"## 4) UFC Fights DATA Snapshot  \n**Purpose:** Visit each fight page and extract structured fight metadata (fighters, weight class, method, rounds, refs, time).  \n**Inputs:** Fight URLs dataframe.  \n**Outputs:** Fight‑level dataset ready for joining with per‑fighter stats.  \n**Notes:** Normalizes “end method” and time formats; defensive parsing for corner cases (NC, DQ, overturned).","metadata":{}},{"cell_type":"code","source":"# === UFC Fights DATA Snapshot ===\n# Wejście: /kaggle/working/UFC_fights_urls.(csv|parquet)\n# Wyjście: /kaggle/working/UFC_fights_data.(csv|parquet)\n\nimport os, sys, logging, re, time\nimport pandas as pd\nimport requests, bs4\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\n\nOUTDIR = \"/kaggle/working\"\nFIGHTS_URLS_CSV = os.path.join(OUTDIR, \"UFC_fights_urls.csv\")\nFIGHTS_URLS_PARQUET = os.path.join(OUTDIR, \"UFC_fights_urls.parquet\")\nFIGHTS_DATA_BASENAME = \"UFC_fights_data\"\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\", stream=sys.stdout)\n\ndef save_df(df: pd.DataFrame, name: str):\n    csv_path = os.path.join(OUTDIR, f\"{name}.csv\")\n    pq_path  = os.path.join(OUTDIR, f\"{name}.parquet\")\n    df.to_csv(csv_path, index=False)\n    try:\n        df.to_parquet(pq_path, index=False)\n    except Exception as e:\n        logging.warning(f\"Parquet save failed ({e}); trying fastparquet…\")\n        df.to_parquet(pq_path, index=False, engine=\"fastparquet\")\n    logging.info(f\"Saved: {csv_path}  &  {pq_path}\")\n\ndef http_session():\n    s = requests.Session()\n    retry = Retry(\n        total=4, backoff_factor=0.7,\n        status_forcelist=(429, 500, 502, 503, 504),\n        allowed_methods=frozenset([\"GET\"])\n    )\n    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n    return s\n\ndef normalize_url(u: str) -> str:\n    return (u or \"\").strip().rstrip(\"/\")\n\ndef get_info(label, soup):\n    for p in soup.select(\"p.b-fight-details__text\"):\n        for tag in p.find_all(\"i\", class_=\"b-fight-details__label\"):\n            if tag.text.strip() == label:\n                nxt = tag.next_sibling\n                while nxt:\n                    if isinstance(nxt, str):\n                        t = nxt.strip()\n                        if t: return t\n                    elif hasattr(nxt, \"get_text\"):\n                        t = nxt.get_text(strip=True)\n                        if t: return t\n                    nxt = nxt.next_sibling\n    return None\n\ndef get_result_details(soup):\n    for p in soup.select(\"p.b-fight-details__text\"):\n        if \"Details:\" in p.text:\n            text = p.get_text(separator=\" \", strip=True)\n            return text.split(\"Details:\")[-1].strip()\n    return None\n\ndef get_event_name(soup):\n    header = soup.select_one(\"h2.b-content__title\")\n    return header.get_text(strip=True).replace(\"Event:\", \"\").strip() if header else None\n\ndef get_winner(f1, f2, fighters):\n    s1 = fighters[0].select_one(\".b-fight-details__person-status\")\n    s2 = fighters[1].select_one(\".b-fight-details__person-status\")\n    t1 = s1.get_text(strip=True).upper() if s1 else \"\"\n    t2 = s2.get_text(strip=True).upper() if s2 else \"\"\n    if t1 == \"W\": return f1\n    if t2 == \"W\": return f2\n    return \"No Contest\"\n\n# --- Load fights URLs ---\nif os.path.exists(FIGHTS_URLS_PARQUET):\n    fights_urls_df = pd.read_parquet(FIGHTS_URLS_PARQUET)\nelif os.path.exists(FIGHTS_URLS_CSV):\n    fights_urls_df = pd.read_csv(FIGHTS_URLS_CSV)\nelse:\n    raise FileNotFoundError(\"Nie znaleziono pliku UFC_fights_urls.(csv|parquet) w /kaggle/working\")\n\nfights_urls_df = fights_urls_df.rename(columns={\"Fight_URL\":\"fight_url\"})\nfights_urls_df[\"fight_url\"] = fights_urls_df[\"fight_url\"].astype(str).str.strip().str.rstrip(\"/\")\nfight_urls = fights_urls_df[\"fight_url\"].dropna().unique().tolist()\n\n# Optional: ogranicz liczbę przy testach\nFIGHT_LIMIT = None  # np. 1000\nif FIGHT_LIMIT is not None:\n    fight_urls = fight_urls[:int(FIGHT_LIMIT)]\n    logging.info(f\"FIGHT_LIMIT active → using first {len(fight_urls)} fights\")\n\nsession = http_session()\nHEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; UFC-AI/1.0)\"}\n\ndef scrape_one(url: str):\n    url = normalize_url(url)\n    try:\n        resp = session.get(url, headers=HEADERS, timeout=15)\n        resp.raise_for_status()\n        soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n\n        fighters = soup.select(\"div.b-fight-details__person\")\n        if len(fighters) != 2:\n            return None\n\n        f1 = fighters[0].select_one(\"h3\")\n        f2 = fighters[1].select_one(\"h3\")\n        f1_name = f1.get_text(strip=True) if f1 else None\n        f2_name = f2.get_text(strip=True) if f2 else None\n\n        a1 = fighters[0].select_one(\"a\")\n        a2 = fighters[1].select_one(\"a\")\n        f1_url = normalize_url(a1[\"href\"]) if a1 and a1.has_attr(\"href\") else None\n        f2_url = normalize_url(a2[\"href\"]) if a2 and a2.has_attr(\"href\") else None\n\n        if not f1_name or not f2_name:\n            return None\n\n        winner = get_winner(f1_name, f2_name, fighters)\n        referee = get_info(\"Referee:\", soup)\n\n        # num_rounds z \"Time format: 5 Rnd (5-5-5-5-5)\"\n        num_rounds = 0\n        tf = next((p for p in soup.select(\"p.b-fight-details__text\") if \"Time format:\" in p.text), None)\n        if tf:\n            m = re.search(r'(\\d+)\\s*Rnd', tf.get_text(\" \", strip=True))\n            if m:\n                try:\n                    num_rounds = int(m.group(1))\n                except ValueError:\n                    num_rounds = 0\n\n        # meta\n        try:\n            finish_round = int(get_info(\"Round:\", soup) or 0)\n        except ValueError:\n            finish_round = 0\n        finish_time = get_info(\"Time:\", soup)\n\n        wt = soup.select_one(\"i.b-fight-details__fight-title\")\n        weight_class = wt.get_text(strip=True).replace(\" Bout\", \"\") if wt else None\n        title_fight = bool(weight_class and \"Title Bout\" in weight_class)\n        gender = \"F\" if (weight_class and \"Women\" in weight_class) else \"M\"\n\n        result = get_info(\"Method:\", soup)\n        result_details = get_result_details(soup)\n        event_name = get_event_name(soup)\n\n        return {\n            \"event_name\": event_name,\n            \"referee\": referee,\n            \"f_1\": f1_name,\n            \"f_2\": f2_name,\n            \"f_1_url\": f1_url,\n            \"f_2_url\": f2_url,\n            \"winner\": winner,\n            \"num_rounds\": num_rounds,\n            \"title_fight\": title_fight,\n            \"weight_class\": weight_class,\n            \"gender\": gender,\n            \"result\": result,\n            \"result_details\": result_details,\n            \"finish_round\": finish_round,\n            \"finish_time\": finish_time,\n            \"fight_url\": url\n        }\n    except Exception:\n        return None\n\n# --- Parallel scrape (bez przesady, żeby być \"uprzejmym\") ---\nMAX_WORKERS = 16  # możesz zwiększyć do 24/32, ale 16 jest zwykle bezpieczne\nresults = []\nwith ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n    futures = [ex.submit(scrape_one, u) for u in fight_urls]\n    for f in tqdm(as_completed(futures), total=len(futures)):\n        rec = f.result()\n        if rec:\n            results.append(rec)\n\n# --- Build DF & save ---\ncols = [\"event_name\",\"referee\",\"f_1\",\"f_2\",\"f_1_url\",\"f_2_url\",\"winner\",\"num_rounds\",\n        \"title_fight\",\"weight_class\",\"gender\",\"result\",\"result_details\",\n        \"finish_round\",\"finish_time\",\"fight_url\"]\nif not results:\n    logging.warning(\"Brak rekordów (results = []).\")\n    fights_df = pd.DataFrame(columns=cols)\nelse:\n    fights_df = pd.DataFrame(results)[cols].copy()\n    fights_df[\"fight_url\"] = fights_df[\"fight_url\"].astype(str).str.strip().str.rstrip(\"/\")\n    fights_df[\"f_1_url\"] = fights_df[\"f_1_url\"].astype(str)\n    fights_df[\"f_2_url\"] = fights_df[\"f_2_url\"].astype(str)\n    fights_df = fights_df.drop_duplicates(subset=[\"fight_url\"]).reset_index(drop=True)\n\nsave_df(fights_df, FIGHTS_DATA_BASENAME)\n\nprint(\"Summary:\")\nprint(\" - UFC_fights_data:\", fights_df.shape)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-04T12:38:02.445728Z","iopub.status.busy":"2025-09-04T12:38:02.445270Z","iopub.status.idle":"2025-09-04T12:50:20.169947Z","shell.execute_reply":"2025-09-04T12:50:20.169018Z","shell.execute_reply.started":"2025-09-04T12:38:02.445682Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 8324/8324 [12:17<00:00, 11.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Summary:\n"," - UFC_fights_data: (8241, 16)\n"]}],"execution_count":4},{"cell_type":"markdown","source":"## 5) Scrape UFC Fight **Stats** (async, aiohttp)  \n**Purpose:** Collect per‑fighter, per‑round statistics from the UFCStats tables for each fight URL, concurrently.  \n**Inputs:** Fight URLs; async concurrency/timeout settings.  \n**Outputs:** Long‑format stats table (fighter x round) covering totals and significant strike breakdowns.  \n**Notes:** Uses aiohttp with semaphore‑controlled concurrency and retries. Robust to partial tables and missing rounds.","metadata":{}},{"cell_type":"code","source":"# %% [markdown]\n# ## Scrape: UFC fight *stats* (async, aiohttp) → CSV + Parquet\n\n# %%\nimport asyncio\nimport aiohttp\nfrom aiohttp import ClientTimeout\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport traceback\nimport re\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\n\n# ---------- Config ----------\nINPUT_FIGHTS_URLS_PARQUET = \"UFC_fights_urls.parquet\"\nINPUT_FIGHTS_URLS_CSV     = \"UFC_fights_urls.csv\"\nOUTPUT_STATS_CSV          = \"UFC_fights_stats_data.csv\"\nOUTPUT_STATS_PARQUET      = \"UFC_fights_stats_data.parquet\"\n\nCONCURRENCY = 8          # równoległość requestów (bezpiecznie dla ufcstats)\nRETRIES     = 3          # próby / URL\nDELAY_RANGE = (0.35, 1.0) # jitter (throttling)\nREQUEST_TIMEOUT = 18\n\n# ---------- I/O: wczytaj listę walk ----------\ndef load_fight_urls() -> pd.DataFrame:\n    if Path(INPUT_FIGHTS_URLS_PARQUET).exists():\n        df = pd.read_parquet(INPUT_FIGHTS_URLS_PARQUET)\n    elif Path(INPUT_FIGHTS_URLS_CSV).exists():\n        df = pd.read_csv(INPUT_FIGHTS_URLS_CSV)\n    else:\n        raise FileNotFoundError(\"Nie znaleziono pliku z linkami walk: \"\n                                f\"{INPUT_FIGHTS_URLS_PARQUET} ani {INPUT_FIGHTS_URLS_CSV}\")\n\n    col = None\n    for c in df.columns:\n        if c.lower() in {\"fight_url\",\"fight url\",\"url\",\"link\"}:\n            col = c\n            break\n    if col is None and \"Fight_URL\" in df.columns:\n        col = \"Fight_URL\"\n    if col is None:\n        # spróbuj po nazwie z poprzednich kroków\n        candidates = [c for c in df.columns if \"fight\" in c.lower() and \"url\" in c.lower()]\n        if not candidates:\n            raise ValueError(\"Nie znalazłem kolumny z URL-ami walk.\")\n        col = candidates[0]\n\n    df[col] = df[col].astype(str).str.strip().str.rstrip(\"/\")\n    df = df.dropna(subset=[col]).drop_duplicates(subset=[col]).reset_index(drop=True)\n    df = df.rename(columns={col: \"fight_url\"})\n    return df[[\"fight_url\"]]\n\n# ---------- Parser utils ----------\ndef time_to_sec(val: str) -> int:\n    try:\n        m, s = map(int, val.strip().split(\":\"))\n        return m * 60 + s\n    except:\n        return 0\n\ndef split_of(txt: str):\n    try:\n        a, b = txt.split(\" of \")\n        return int(a.strip()), int(b.strip())\n    except:\n        return 0, 0\n\ndef to_int(txt: str) -> int:\n    try:\n        return int(txt.strip().replace(\"%\",\"\").replace(\"---\",\"0\"))\n    except:\n        return 0\n\ndef find_summary_table(soup: BeautifulSoup):\n    p_tag = soup.find(\"p\", class_=\"b-fight-details__collapse-link_tot\", string=re.compile(\"Totals\", re.IGNORECASE))\n    if not p_tag:\n        return None\n    return p_tag.find_next(\"table\")\n\ndef parse_round_table(soup: BeautifulSoup, idx: int, f1: str, f2: str,\n                      url: str, u1: str, u2: str, cols: list) -> pd.DataFrame:\n    tables = soup.select(\"table.b-fight-details__table.js-fight-table\")\n    if idx >= len(tables):\n        return pd.DataFrame()\n    rows = tables[idx].find_all([\"thead\", \"tr\"])\n    round_n = 0\n    out = []\n\n    for row in rows:\n        if row.name == \"thead\" and \"Round\" in row.text:\n            match = re.search(r\"Round (\\d+)\", row.text)\n            if match:\n                round_n = int(match.group(1))\n            continue\n        if row.name != \"tr\":\n            continue\n\n        tds = row.find_all(\"td\")\n        if not tds or len(tds) < len(cols):\n            continue\n\n        p_tags_by_td = [td.find_all(\"p\") for td in tds]\n        for i, (f_name, f_url) in enumerate([(f1, u1), (f2, u2)]):\n            r = {\n                \"fighter_id\": f_name,\n                \"fighter_url\": f_url,\n                \"fight_url\": url,\n                \"round\": round_n\n            }\n            for j, key in enumerate(cols):\n                try:\n                    txt = p_tags_by_td[j+1][i].text.strip()\n                    if \"of\" in txt:\n                        a, b = txt.split(\" of \")\n                        r[f\"{key}_succ\"] = int(a.strip())\n                        r[f\"{key}_att\"]  = int(b.strip())\n                    elif \":\" in txt:\n                        r[key] = time_to_sec(txt)\n                    else:\n                        r[key] = to_int(txt)\n                except:\n                    r[key] = 0\n            out.append(r)\n\n    return pd.DataFrame(out)\n\ndef get_texts_from_summary_row(summary_tds):\n    # summary_tds to lista <td> w pierwszym wierszu sekcji Totals\n    def get_p(col_idx, i):\n        try:\n            return summary_tds[col_idx].find_all(\"p\")[i].text.strip()\n        except:\n            return \"0\"\n    return get_p\n\nasync def fetch_fight(session, url, sem,\n                      delay_range=DELAY_RANGE,\n                      max_retries=RETRIES) -> list[dict]:\n    async with sem:\n        for attempt in range(max_retries):\n            try:\n                await asyncio.sleep(random.uniform(*delay_range))\n                async with session.get(url, timeout=ClientTimeout(total=REQUEST_TIMEOUT)) as res:\n                    html = await res.text()\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    # identyfikacja zawodników\n                    names = soup.select(\"a.b-fight-details__person-link\")\n                    if len(names) < 2:\n                        return []\n\n                    f1, f2 = names[0].text.strip(), names[1].text.strip()\n                    u1, u2 = names[0].get(\"href\",\"\").strip().rstrip(\"/\"), names[1].get(\"href\",\"\").strip().rstrip(\"/\")\n\n                    # tabela Totals\n                    summary_table = find_summary_table(soup)\n                    if not summary_table:\n                        return []\n\n                    t_body_rows = summary_table.select(\"tbody tr\")\n                    if not t_body_rows:\n                        return []\n\n                    cols = t_body_rows[0].find_all(\"td\")\n                    get_p = get_texts_from_summary_row(cols)\n\n                    # rekordy „Totals” per fighter\n                    records = []\n                    for i, (f_name, f_url) in enumerate([(f1,u1),(f2,u2)]):\n                        sig_succ, sig_att = split_of(get_p(2, i))\n                        tot_succ, tot_att = split_of(get_p(4, i))\n                        td_succ,  td_att  = split_of(get_p(5, i))\n\n                        rec = {\n                            \"fighter_id\": f_name,\n                            \"fighter_url\": f_url,\n                            \"fight_url\": url,\n                            \"knockdowns\": to_int(get_p(1, i)),\n                            \"sig_strikes_succ\": sig_succ,\n                            \"sig_strikes_att\": sig_att,\n                            \"total_strikes_succ\": tot_succ,\n                            \"total_strikes_att\": tot_att,\n                            \"takedown_succ\": td_succ,\n                            \"takedown_att\": td_att,\n                            \"submission_att\": to_int(get_p(7, i)),\n                            \"reversals\": to_int(get_p(8, i)),\n                            \"ctrl_time_sec\": time_to_sec(get_p(9, i)),\n                        }\n                        records.append(rec)\n\n                    # tabele rundowe\n                    df_totals = parse_round_table(\n                        soup, 0, f1, f2, url, u1, u2,\n                        [\"knockdowns\",\"sig_strikes\",\"sig_pct\",\"total_strikes\",\"td_1\",\"td_2\",\"submission_att\",\"reversals\",\"ctrl\"]\n                    )\n                    df_sig = parse_round_table(\n                        soup, 1, f1, f2, url, u1, u2,\n                        [\"sig_pct\",\"sig_strikes\",\"head\",\"body\",\"leg\",\"distance\",\"clinch\",\"ground\"]\n                    )\n\n                    # index po (fighter_url, round) dla łatwego łączenia\n                    if not df_totals.empty:\n                        df_totals = df_totals.set_index([\"fighter_url\",\"round\"])\n                    if not df_sig.empty:\n                        df_sig = df_sig.set_index([\"fighter_url\",\"round\"])\n\n                    # wzbogacenie rekordów totals o rundy 1..5\n                    enriched = []\n                    all_keys = set()\n                    for rec in records:\n                        f_url = rec[\"fighter_url\"]\n                        base = rec.copy()\n                        for r in range(1, 6):\n                            row_t = df_totals.loc[(f_url, r)].to_dict() if (not df_totals.empty and (f_url, r) in df_totals.index) else {}\n                            row_s = df_sig.loc[(f_url, r)].to_dict() if (not df_sig.empty and (f_url, r) in df_sig.index) else {}\n\n                            for k, v in row_t.items():\n                                col = f\"round{r}_{k}\"\n                                base[col] = v\n                                all_keys.add(col)\n                            for k, v in row_s.items():\n                                col = f\"round{r}_{k}\"\n                                base[col] = v\n                                all_keys.add(col)\n\n                        # dopełnij brakujące kolumny zerami\n                        for col in all_keys:\n                            base.setdefault(col, 0)\n\n                        enriched.append(base)\n\n                    return enriched\n\n            except Exception:\n                # log lokalny; w notebooku nie rozwlekamy outputu\n                await asyncio.sleep(1.5 * (attempt + 1))\n\n        # po RETRIES się poddajemy\n        return []\n\nasync def scrape_stats_async(urls: list[str]) -> list[dict]:\n    sem = asyncio.Semaphore(CONCURRENCY)\n    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; Kaggle-UFC-Scraper/1.0)\"}\n    timeout = ClientTimeout(total=REQUEST_TIMEOUT)\n\n    results = []\n    connector = aiohttp.TCPConnector(limit=CONCURRENCY*3, enable_cleanup_closed=True)\n\n    async with aiohttp.ClientSession(headers=headers, timeout=timeout, connector=connector) as session:\n        tasks = [fetch_fight(session, u, sem) for u in urls]\n        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Scraping fights (stats)\"):\n            res = await f\n            if res:\n                results.extend(res)\n    return results\n","metadata":{"execution":{"iopub.execute_input":"2025-09-04T12:53:12.457889Z","iopub.status.busy":"2025-09-04T12:53:12.457591Z","iopub.status.idle":"2025-09-04T12:53:12.980439Z","shell.execute_reply":"2025-09-04T12:53:12.979569Z","shell.execute_reply.started":"2025-09-04T12:53:12.457866Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 6) Orchestrate: Stats Job Runner  \n**Purpose:** Load fight URLs, spawn the async stats scraper, and combine outputs into a single dataframe.  \n**Inputs:** Source paths produced by earlier cells; concurrency controls.  \n**Outputs:** Final fight‑stats dataframe; optional CSV/Parquet persistence.  \n**Notes:** Includes fallback to `nest_asyncio` for Kaggle/Notebook event‑loop edge cases.","metadata":{}},{"cell_type":"code","source":"# %%\n# 1) urls\ndf_urls = load_fight_urls()\nfight_urls = df_urls[\"fight_url\"].tolist()\nprint(f\"Fight URLs total: {len(fight_urls)}\")\n# --- notebook-safe async runner ---\ndef run_async_notebook_safe(coro):\n    try:\n        return asyncio.run(coro)\n    except RuntimeError as e:\n        # Jupyter/Kaggle ma już działający event loop\n        if \"asyncio.run() cannot be called from a running event loop\" in str(e):\n            try:\n                import nest_asyncio  # zwykle dostępne w Kaggle\n                nest_asyncio.apply()\n            except Exception:\n                pass\n            loop = asyncio.get_event_loop()\n            return loop.run_until_complete(coro)\n        raise\n# 2) run\nstart = time.time()\nrecords = run_async_notebook_safe(scrape_stats_async(fight_urls))\nelapsed = time.time() - start\n\n# 3) do DataFrame\nif not records:\n    raise RuntimeError(\"Brak zebranych rekordów – sprawdź połączenie lub ogranicz CONCURRENCY.\")\n\ndf_stats = pd.DataFrame(records).drop_duplicates()\n\n# czyszczenie podstawowych typów\nint_like = [\n    \"knockdowns\",\"sig_strikes_succ\",\"sig_strikes_att\",\"total_strikes_succ\",\"total_strikes_att\",\n    \"takedown_succ\",\"takedown_att\",\"submission_att\",\"reversals\",\"ctrl_time_sec\"\n]\nfor c in int_like:\n    if c in df_stats.columns:\n        df_stats[c] = pd.to_numeric(df_stats[c], errors=\"coerce\").fillna(0).astype(int)\n\n# 4) zapis\ndf_stats.to_csv(OUTPUT_STATS_CSV, index=False)\ndf_stats.to_parquet(OUTPUT_STATS_PARQUET, index=False)\n\n# 5) szybkie podsumowanie\nn_fights = df_stats[\"fight_url\"].nunique()\nn_rows   = len(df_stats)\nprint(f\"\\n100%|{'█'*10}| {len(fight_urls)}/{len(fight_urls)} [{elapsed/60:0.2f} min]\")\nprint(\"Summary:\")\nprint(f\" - UFC_fights_stats_data: ({n_rows:,}, {df_stats.shape[1]})\")\nprint(f\" - Unique fights covered: {n_fights:,} / expected {len(fight_urls):,}\")\nprint(f\"Saved → {OUTPUT_STATS_CSV}  |  {OUTPUT_STATS_PARQUET}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-09-04T12:56:18.086491Z","iopub.status.busy":"2025-09-04T12:56:18.086172Z","iopub.status.idle":"2025-09-04T13:20:00.105262Z","shell.execute_reply":"2025-09-04T13:20:00.104188Z","shell.execute_reply.started":"2025-09-04T12:56:18.086464Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Fight URLs total: 8324\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f18c33b30cf46f8ab61ab50d7a75219","version_major":2,"version_minor":0},"text/plain":["Scraping fights (stats):   0%|          | 0/8324 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","100%|██████████| 8324/8324 [23.66 min]\n","Summary:\n"," - UFC_fights_stats_data: (16,606, 158)\n"," - Unique fights covered: 8,303 / expected 8,324\n","Saved → UFC_fights_stats_data.csv  |  UFC_fights_stats_data.parquet\n"]}],"execution_count":8},{"cell_type":"markdown","source":"## 7) UFC Fighters A–Z — URLs Snapshot  \n**Purpose:** Enumerate all fighter profile URLs (alphabetical index) from UFCStats.  \n**Inputs:** A–Z listing pages.  \n**Outputs:** Dataframe of fighter profile URLs with slug/ID fields for joins.  \n**Notes:** Skips dupes; resilient to occasional 404s or soft‑redirects.","metadata":{}},{"cell_type":"code","source":"# === UFC Fighters URLs Snapshot (A–Z) ===\n# Wyjście: /kaggle/working/UFC_fighters_urls.(csv|parquet)\n\nimport os, sys, logging, time, re\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport pandas as pd\nfrom tqdm import tqdm\n\nOUTDIR = Path(\"/kaggle/working\")\nOUTDIR.mkdir(exist_ok=True)\nBASENAME = \"UFC_fighters_urls\"\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\", stream=sys.stdout)\n\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")  # możesz ograniczyć do testów, np. [\"a\",\"b\"]\n\ndef http_session():\n    s = requests.Session()\n    retry = Retry(\n        total=4, backoff_factor=0.8,\n        status_forcelist=(429, 500, 502, 503, 504),\n        allowed_methods=frozenset([\"GET\"])\n    )\n    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n    return s\n\nSESSION = http_session()\nHEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; Kaggle-UFC-Scraper/1.0)\"}\n\ndef fetch_letter(letter, retries=3, delay=5):\n    url = f\"http://ufcstats.com/statistics/fighters?char={letter}&page=all\"\n    for attempt in range(retries):\n        try:\n            r = SESSION.get(url, headers=HEADERS, timeout=15)\n            if r.status_code == 429:\n                time.sleep(delay)\n                continue\n            r.raise_for_status()\n            return letter, r.text\n        except Exception:\n            time.sleep(delay)\n    return letter, None\n\ndef extract_fighter_urls_from_html(html: str):\n    soup = BeautifulSoup(html, \"html.parser\")\n    # celujemy w linki \"fighter-details\" (najbardziej stabilny selektor)\n    links = [a[\"href\"].strip().rstrip(\"/\") for a in soup.select('a.b-link[href*=\"fighter-details\"]')]\n    # czasem na stronie jest po kilka linków do tego samego fightera → dedup\n    return list(dict.fromkeys(links))\n\ndef save_df(df: pd.DataFrame, name: str):\n    csv_path = OUTDIR / f\"{name}.csv\"\n    pq_path  = OUTDIR / f\"{name}.parquet\"\n    df.to_csv(csv_path, index=False)\n    try:\n        df.to_parquet(pq_path, index=False)\n    except Exception as e:\n        logging.warning(f\"Parquet save failed ({e}); trying fastparquet…\")\n        df.to_parquet(pq_path, index=False, engine=\"fastparquet\")\n    logging.info(f\"Saved: {csv_path}  &  {pq_path}\")\n\nall_urls = []\n\nwith ThreadPoolExecutor(max_workers=6) as ex:\n    futures = [ex.submit(fetch_letter, L) for L in LETTERS]\n    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching A–Z pages\"):\n        letter, html = fut.result()\n        if html:\n            urls = extract_fighter_urls_from_html(html)\n            all_urls.extend(urls)\n\nall_urls = list(dict.fromkeys(all_urls))  # dedup z zachowaniem kolejności\n\ndf_furls = pd.DataFrame({\"fighter_url\": all_urls})\ndf_furls[\"fighter_url\"] = df_furls[\"fighter_url\"].astype(str).str.strip().str.rstrip(\"/\")\ndf_furls[\"scraped_at\"] = datetime.now(timezone.utc).isoformat()\n\nsave_df(df_furls, BASENAME)\n\nprint(\"Summary:\")\nprint(\" - UFC_fighters_urls:\", df_furls.shape)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-05T10:45:49.596898Z","iopub.status.busy":"2025-09-05T10:45:49.596067Z","iopub.status.idle":"2025-09-05T10:45:55.268337Z","shell.execute_reply":"2025-09-05T10:45:55.267413Z","shell.execute_reply.started":"2025-09-05T10:45:49.596860Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["Fetching A–Z pages: 100%|██████████| 26/26 [00:05<00:00,  4.64it/s]"]},{"name":"stdout","output_type":"stream","text":["Summary:\n"," - UFC_fighters_urls: (4436, 2)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"execution_count":2},{"cell_type":"markdown","source":"## 8) UFC Fighter Profiles — Adaptive & Resumable (async)  \n**Purpose:** Scrape individual fighter profile pages for bio and career record, concurrently. Supports resume on failure.  \n**Inputs:** Fighter profile URLs; concurrency/timeout settings.  \n**Outputs:** Fighter‑level table (name, record, height/reach, stance, DOB, team, etc.) with normalized types.  \n**Notes:** Adaptive retries and soft parsing for partially filled profiles; checkpointing/resume logic.","metadata":{}},{"cell_type":"code","source":"# === UFC Fighters PROFILE SCRAPER — adaptive + resumable ===\n# In : /kaggle/working/UFC_fighters_urls.(csv|parquet)\n# Out: /kaggle/working/UFC_fighters_data.csv | .parquet  (+ shardy w /kaggle/working/_fighters_shards)\n\nimport asyncio, aiohttp, random, re, time, math, os\nfrom aiohttp import ClientTimeout\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# ---------- PATHS ----------\nWORK = Path(\"/kaggle/working\")\nURLS_PQ  = WORK/\"UFC_fighters_urls.parquet\"\nURLS_CSV = WORK/\"UFC_fighters_urls.csv\"\nFINAL_CSV= WORK/\"UFC_fighters_data.csv\"\nFINAL_PQ = WORK/\"UFC_fighters_data.parquet\"\nSHARDS   = WORK/\"_fighters_shards\"; SHARDS.mkdir(exist_ok=True)\n\n# ---------- PARAMS (startowe; skorygują się automatycznie) ----------\nBATCH_SIZE   = 300\nCONCURRENCY  = 5            # start\nDELAY_RANGE  = (0.18, 0.42) # start\nTIMEOUT      = 25\nRETRIES      = 6\nMIN_CONC     = 2\nMAX_CONC     = 8\n\nUA_POOL = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n]\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\ndef headers():\n    return {\n        \"User-Agent\": random.choice(UA_POOL),\n        \"Accept\": \"text/html,application/xhtml+xml\",\n        \"Accept-Language\": random.choice([\"en-US,en;q=0.9\",\"en-GB,en;q=0.8\",\"en;q=0.7\"]),\n        \"Connection\": \"keep-alive\",\n        \"Referer\": f\"http://ufcstats.com/statistics/fighters?char={random.choice(LETTERS)}&page=all\",\n    }\n\n# ---------- PARSERY ----------\ndef parse_name(soup):\n    t = soup.select_one(\"div.b-content__title h2\") or soup.select_one(\"span.b-content__title-highlight\")\n    txt = t.get_text(\" \", strip=True) if t else \"\"\n    parts = [p for p in txt.split() if p]\n    first = parts[0] if parts else None\n    last  = \" \".join(parts[1:]) if len(parts)>1 else \"NULL\"\n    return first, last\n\ndef parse_record(soup):\n    rec = soup.select_one(\"span.b-content__title-record\")\n    txt = rec.get_text(\" \", strip=True) if rec else \"\"\n    m = re.search(r\"(\\d+)-(\\d+)-(\\d+)\\s*(?:\\((\\d+)\\s*NC\\))?\", txt)\n    if m:\n        w,l,d = int(m.group(1)), int(m.group(2)), int(m.group(3))\n        nc = int(m.group(4)) if m.group(4) else 0\n        return w,l,d,nc\n    nums = [int(x) for x in re.findall(r\"\\d+\", txt)]\n    w = nums[0] if len(nums)>0 else 0\n    l = nums[1] if len(nums)>1 else 0\n    d = nums[2] if len(nums)>2 else 0\n    nc= nums[3] if len(nums)>3 else 0\n    return w,l,d,nc\n\ndef parse_dims_block(soup):\n    height_cm = weight_lbs = reach_cm = None\n    stance = \"NULL\"; dob = None\n    for li in soup.select(\"li.b-list__box-list-item\"):\n        t = li.get_text(\" \", strip=True)\n        if t.startswith(\"Height:\"):\n            m = re.search(r\"(\\d+)\\s*'\\s*(\\d+)\\s*\\\"\", t)\n            if m: height_cm = int(round((int(m.group(1))*12 + int(m.group(2))) * 2.54))\n        elif t.startswith(\"Weight:\"):\n            m = re.search(r\"(\\d+)\", t)\n            if m: weight_lbs = int(m.group(1))\n        elif t.startswith(\"Reach:\"):\n            m = re.search(r\"(\\d+(?:\\.\\d+)?)\", t)\n            if m: reach_cm = int(round(float(m.group(1))*2.54))\n        elif t.upper().startswith(\"STANCE:\") or t.startswith(\"Stance:\"):\n            stance = t.split(\":\",1)[1].strip() or \"NULL\"\n        elif t.startswith(\"DOB:\"):\n            raw = t.split(\":\",1)[1].strip()\n            if raw and raw != \"--\":\n                from datetime import datetime\n                for fmt in (\"%b %d, %Y\", \"%B %d, %Y\"):\n                    try:\n                        dob = datetime.strptime(raw, fmt).strftime(\"%Y-%m-%d\"); break\n                    except: pass\n    return height_cm, weight_lbs, reach_cm, stance, dob\n\ndef parse_career_stats(soup):\n    out = { \"fighter_SlpM\":0.0, \"fighter_Str_Acc\":0.0, \"fighter_SApM\":0.0, \"fighter_Str_Def\":0.0,\n            \"fighter_TD_Avg\":0.0, \"fighter_TD_Acc\":0.0, \"fighter_TD_Def\":0.0, \"fighter_Sub_Avg\":0.0 }\n    for li in soup.select(\"ul.b-list__box-list.b-list__box-list_margin-top li.b-list__box-list-item\"):\n        txt = li.get_text(\" \", strip=True)\n        if \":\" not in txt: continue\n        k, v = [x.strip() for x in txt.split(\":\",1)]\n        def pct(x):\n            m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*%\", x); return float(m.group(1))/100.0 if m else 0.0\n        def f(x): \n            try: return 0.0 if x in [\"--\",\"\"] else float(x)\n            except: return 0.0\n        if   k == \"SLpM\":       out[\"fighter_SlpM\"] = f(v)\n        elif k == \"Str. Acc.\":  out[\"fighter_Str_Acc\"] = pct(v)\n        elif k == \"SApM\":       out[\"fighter_SApM\"] = f(v)\n        elif k in (\"Str. Def\",\"Str. Def.\"): out[\"fighter_Str_Def\"] = pct(v)\n        elif k == \"TD Avg.\":    out[\"fighter_TD_Avg\"] = f(v)\n        elif k == \"TD Acc.\":    out[\"fighter_TD_Acc\"] = pct(v)\n        elif k == \"TD Def.\":    out[\"fighter_TD_Def\"] = pct(v)\n        elif k == \"Sub. Avg.\":  out[\"fighter_Sub_Avg\"] = f(v)\n    return out\n\n# ---------- IO ----------\ndef load_seed():\n    if URLS_PQ.exists(): df = pd.read_parquet(URLS_PQ)\n    elif URLS_CSV.exists(): df = pd.read_csv(URLS_CSV)\n    else: raise FileNotFoundError(\"Brak UFC_fighters_urls.(csv|parquet)\")\n    col = \"fighter_url\" if \"fighter_url\" in df.columns else [c for c in df.columns if \"url\" in c.lower()][0]\n    urls = (df[col].astype(str).str.strip().str.rstrip(\"/\").dropna().unique().tolist())\n    return urls\n\ndef load_scraped_set():\n    # pozwala na wznowienie\n    scraped = set()\n    if FINAL_PQ.exists(): scraped |= set(pd.read_parquet(FINAL_PQ)[\"fighter_url\"].astype(str))\n    elif FINAL_CSV.exists(): scraped |= set(pd.read_csv(FINAL_CSV)[\"fighter_url\"].astype(str))\n    for p in SHARDS.glob(\"fighters_part_*.parquet\"):\n        try:\n            scraped |= set(pd.read_parquet(p)[\"fighter_url\"].astype(str))\n        except: pass\n    return set([u.strip().rstrip(\"/\") for u in scraped])\n\ndef save_shard(df, idx):\n    out = SHARDS/f\"fighters_part_{idx:04d}.parquet\"\n    df.to_parquet(out, index=False)\n\ndef finalize_merge():\n    parts = sorted(SHARDS.glob(\"fighters_part_*.parquet\"))\n    dfs = [pd.read_parquet(p) for p in parts] if parts else []\n    if FINAL_PQ.exists():\n        dfs.insert(0, pd.read_parquet(FINAL_PQ))\n    elif FINAL_CSV.exists():\n        dfs.insert(0, pd.read_csv(FINAL_CSV))\n    if not dfs:\n        raise RuntimeError(\"Brak danych do scalenia.\")\n    full = (pd.concat(dfs, ignore_index=True)\n              .drop_duplicates(subset=[\"fighter_url\"])\n              .reset_index(drop=True))\n    full.to_csv(FINAL_CSV, index=False)\n    try:\n        full.to_parquet(FINAL_PQ, index=False)\n    except Exception:\n        full.to_parquet(FINAL_PQ, index=False, engine=\"fastparquet\")\n    return full\n\n# ---------- FETCH ----------\nasync def fetch_one(session, sem, url, delay_range, retries):\n    url = url.strip().rstrip(\"/\")\n    async with sem:\n        backoff = 1.1\n        for attempt in range(retries):\n            try:\n                await asyncio.sleep(random.uniform(*delay_range))\n                async with session.get(url, headers=headers()) as resp:\n                    if resp.status in (429,403,500,502,503,504):\n                        await asyncio.sleep(backoff*(attempt+1)); continue\n                    resp.raise_for_status()\n                    html = await resp.text()\n                    if not html or len(html) < 1500:\n                        await asyncio.sleep(backoff*(attempt+1)); continue\n                    soup = BeautifulSoup(html, \"html.parser\")\n\n                    f_name, l_name = parse_name(soup)\n                    if not f_name:\n                        await asyncio.sleep(backoff*(attempt+1)); continue\n\n                    nn = soup.select_one(\"p.b-content__Nickname\")\n                    nickname = nn.get_text(strip=True) if nn and nn.get_text(strip=True) else \"NULL\"\n                    h,w,r,stance,dob = parse_dims_block(soup)\n                    W,L,D,NC = parse_record(soup)\n                    stats = parse_career_stats(soup)\n\n                    rec = {\n                        \"fighter_f_name\": f_name,\n                        \"fighter_l_name\": l_name if l_name else \"NULL\",\n                        \"fighter_nickname\": nickname,\n                        \"fighter_height_cm\": h,\n                        \"fighter_weight_lbs\": w,\n                        \"fighter_reach_cm\": r,\n                        \"fighter_stance\": stance if stance else \"NULL\",\n                        \"fighter_dob\": dob,\n                        \"fighter_w\": W, \"fighter_l\": L, \"fighter_d\": D, \"fighter_nc_dq\": NC,\n                        **stats,\n                        \"fighter_url\": url\n                    }\n                    return rec\n            except Exception:\n                await asyncio.sleep(backoff*(attempt+1))\n    return None\n\nasync def scrape_batch(urls, concurrency, delay_range):\n    timeout = ClientTimeout(total=TIMEOUT)\n    connector = aiohttp.TCPConnector(limit=concurrency*2, limit_per_host=concurrency, enable_cleanup_closed=True)\n    sem = asyncio.Semaphore(concurrency)\n    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:\n        tasks = [fetch_one(session, sem, u, delay_range, RETRIES) for u in urls]\n        out = []\n        for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f\"Batch (C={concurrency}, delay={delay_range})\"):\n            rec = await fut\n            if rec: out.append(rec)\n        return out\n\ndef nb_run(coro):\n    try:\n        return asyncio.run(coro)\n    except RuntimeError:\n        import nest_asyncio; nest_asyncio.apply()\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(coro)\n\n# ---------- MAIN ----------\nall_urls = load_seed()\nscraped = load_scraped_set()\ntodo = [u for u in all_urls if u not in scraped]\nprint(f\"Total URLs : {len(all_urls)}\")\nprint(f\"Already have: {len(scraped)}\")\nprint(f\"To scrape  : {len(todo)}\")\n\nbatch_idx = 0\nconcur, dly = CONCURRENCY, list(DELAY_RANGE)\n\nwhile todo:\n    sub = todo[:BATCH_SIZE]; todo = todo[BATCH_SIZE:]\n    print(f\"\\n▶️ Batch {batch_idx+1}: {len(sub)} URLs | C={concur}, delay=({dly[0]:.2f},{dly[1]:.2f})\")\n\n    t0 = time.time()\n    recs = nb_run(scrape_batch(sub, concur, tuple(dly)))\n    dt = time.time()-t0\n\n    df = pd.DataFrame(recs)\n    succ = len(df); rate = succ / max(1,len(sub))\n    print(f\"   Collected {succ}/{len(sub)} ({rate*100:.1f}%) in {dt:.1f}s\")\n    if succ:\n        # szybkie czyszczenie typów (bez forsowania Int64)\n        for c in [\"fighter_height_cm\",\"fighter_weight_lbs\",\"fighter_reach_cm\",\"fighter_w\",\"fighter_l\",\"fighter_d\",\"fighter_nc_dq\"]:\n            if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n        for c in [\"fighter_SlpM\",\"fighter_Str_Acc\",\"fighter_SApM\",\"fighter_Str_Def\",\"fighter_TD_Avg\",\"fighter_TD_Acc\",\"fighter_TD_Def\",\"fighter_Sub_Avg\"]:\n            if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n        save_shard(df, batch_idx)\n    else:\n        print(\"   (empty batch)\")\n\n    # adaptacja: jeśli <30% trafień → zwolnij; jeśli >80% → przyspiesz trochę\n    if rate < 0.30:\n        concur = max(MIN_CONC, concur-1)\n        dly[0] = min(dly[0]*1.4, 0.8)\n        dly[1] = min(dly[1]*1.4, 1.2)\n        print(f\"   ↓ Adapting: C={concur}, delay=({dly[0]:.2f},{dly[1]:.2f})\")\n        time.sleep(10)  # krótki chill po słabym batchu\n    elif rate > 0.80 and concur < MAX_CONC:\n        concur += 1\n        dly[0] = max(0.10, dly[0]*0.9)\n        dly[1] = max(0.18, dly[1]*0.9)\n        print(f\"   ↑ Adapting: C={concur}, delay=({dly[0]:.2f},{dly[1]:.2f})\")\n\n    batch_idx += 1\n\n# ---------- MERGE ----------\nfinal = finalize_merge()\nprint(\"\\nSummary:\")\nprint(\" - UFC_fighters_data:\", final.shape)\nprint(\" - Saved →\", FINAL_CSV.name, \"|\", FINAL_PQ.name)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-05T11:28:35.045282Z","iopub.status.busy":"2025-09-05T11:28:35.044914Z","iopub.status.idle":"2025-09-05T11:32:33.497843Z","shell.execute_reply":"2025-09-05T11:32:33.496931Z","shell.execute_reply.started":"2025-09-05T11:28:35.045259Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Total URLs : 4436\n","Already have: 1201\n","To scrape  : 3235\n","\n","▶️ Batch 1: 300 URLs | C=5, delay=(0.18,0.42)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85eccfb579944a8f8aea3b6243984458","version_major":2,"version_minor":0},"text/plain":["Batch (C=5, delay=(0.18, 0.42)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 34.8s\n","   ↑ Adapting: C=6, delay=(0.16,0.38)\n","\n","▶️ Batch 2: 300 URLs | C=6, delay=(0.16,0.38)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff02d4b4e0854770adc2e1a5ef779c6d","version_major":2,"version_minor":0},"text/plain":["Batch (C=6, delay=(0.162, 0.378)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 26.9s\n","   ↑ Adapting: C=7, delay=(0.15,0.34)\n","\n","▶️ Batch 3: 300 URLs | C=7, delay=(0.15,0.34)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bef5db7285a49179737db7c2f3f26e5","version_major":2,"version_minor":0},"text/plain":["Batch (C=7, delay=(0.1458, 0.3402)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 22.7s\n","   ↑ Adapting: C=8, delay=(0.13,0.31)\n","\n","▶️ Batch 4: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1110bde31d1494ebc6c1bb50ba83802","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 21.2s\n","\n","▶️ Batch 5: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"110ebdf4ae5e4937925c67a883576ef3","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 18.4s\n","\n","▶️ Batch 6: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c779180c96b4d72a932d3650010f15c","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 19.2s\n","\n","▶️ Batch 7: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d6c593934ab4f2098f21a258e276802","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 20.9s\n","\n","▶️ Batch 8: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c507bce4508643a3b2b3c56e81d11b73","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 18.7s\n","\n","▶️ Batch 9: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d296eb04a42c42299f92f35ca32a8ef2","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 19.6s\n","\n","▶️ Batch 10: 300 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"374de1e51ccf40f48b27010676a68ce5","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/300 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 300/300 (100.0%) in 21.0s\n","\n","▶️ Batch 11: 235 URLs | C=8, delay=(0.13,0.31)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b156e65924a64bf59b3bf614eb3ac610","version_major":2,"version_minor":0},"text/plain":["Batch (C=8, delay=(0.13122, 0.30618)):   0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["   Collected 235/235 (100.0%) in 14.7s\n","\n","Summary:\n"," - UFC_fighters_data: (4436, 21)\n"," - Saved → UFC_fighters_data.csv | UFC_fighters_data.parquet\n"]}],"execution_count":10},{"cell_type":"markdown","source":"## 9) Betting Odds — The Odds API → CSV & Parquet  \n**Purpose:** Pull current and historical moneyline odds for UFC events using The Odds API.  \n**Inputs:** The Odds API key (use environment variable in Kaggle `os.environ`), market/sport params.  \n**Outputs:** Odds table with book, price, timestamp; saved to `/kaggle/working` in CSV and Parquet.  \n**Notes:** Respect rate limits; never hard‑code private keys in public notebooks.","metadata":{}},{"cell_type":"code","source":"# === UFC Betting Odds (The Odds API) → CSV & Parquet ===\n# In : Internet must be ON (Kaggle: enable “Internet”)\n# Out: /kaggle/working/UFC_betting_odds_api.csv  |  .parquet\n#\n# About the API key (important):\n# - Do NOT hardcode your private key in public code. Read it from an environment\n#   variable instead (e.g., Kaggle → Add-ons → Secrets → create ODDS_API_KEY).\n# - This is NOT an endorsement. The Odds API has historically offered a free tier\n#   of ~500 requests/month, which is typically enough for personal snapshots.\n#   Policies can change — check their site for current limits.\n# - Writing individual scrapers for each bookmaker is brittle/time-consuming; this\n#   API is used here purely as a pragmatic, single-source snapshot.\n\nimport os, time, json, datetime as dt\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nimport requests\nimport pandas as pd\n\n# --- konfig ---\nOUTDIR   = Path(\"/kaggle/working\"); OUTDIR.mkdir(exist_ok=True)\nBASENAME = \"UFC_betting_odds_api\"\nCSV_PATH = OUTDIR / f\"{BASENAME}.csv\"\nPQ_PATH  = OUTDIR / f\"{BASENAME}.parquet\"\n\n# API KEY: weź z env, a jak nie ma — użyj podanego klucza (uwaga: klucz w kodzie = ryzyko wycieku)\nAPI_KEY = os.getenv(\"ODDS_API_KEY\", \"\")\n\nAPI_URL = \"https://api.the-odds-api.com/v4/sports/mma_mixed_martial_arts/odds\"\nPARAMS = {\n    \"regions\": \"us,uk,eu\",\n    \"markets\": \"h2h\",\n    \"oddsFormat\": \"decimal\",\n    \"apiKey\": API_KEY\n}\n\nBOOKMAKER_REGIONS = {\n    \"DraftKings\": \"us\", \"FanDuel\": \"us\", \"BetMGM\": \"us\", \"BetUS\": \"us\", \"BetOnline.ag\": \"us\", \"BetRivers\": \"us\", \"BetAnySports\": \"us\",\n    \"888sport\": \"uk\", \"Betfair\": \"uk\", \"Betway\": \"uk\", \"Paddy Power\": \"uk\", \"Virgin Bet\": \"uk\", \"Grosvenor\": \"uk\", \"LiveScore Bet\": \"uk\", \"Matchbook\": \"uk\",\n    \"Unibet\": \"eu\", \"Unibet (FR)\": \"eu\", \"Unibet (NL)\": \"eu\", \"Betclic (FR)\": \"eu\", \"LeoVegas\": \"eu\", \"Marathon Bet\": \"eu\", \"Nordic Bet\": \"eu\", \"Coolbet\": \"eu\", \"Betsson\": \"eu\"\n}\n\n# --- pobranie z prostym retry ---\ndef fetch_odds(url: str, params: Dict[str, Any], retries: int = 3, backoff: float = 1.2):\n    last_headers = {}\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, params=params, timeout=20)\n            last_headers = r.headers\n            if r.status_code == 200:\n                return r.json(), last_headers\n            # 429 / 5xx: spróbuj ponownie\n            if r.status_code in (429, 500, 502, 503, 504):\n                time.sleep(backoff * (attempt + 1))\n                continue\n            # inne kody → przerwij\n            raise RuntimeError(f\"HTTP {r.status_code}: {r.text[:300]}\")\n        except requests.RequestException as e:\n            if attempt == retries - 1:\n                raise\n            time.sleep(backoff * (attempt + 1))\n    # jeśli tu dotarliśmy, coś nie tak\n    raise RuntimeError(f\"Fetch failed after {retries} attempts; last headers: {last_headers}\")\n\ndata, hdrs = fetch_odds(API_URL, PARAMS)\n\n# pokaż limity API (jeśli zwrócone)\nrl_rem = hdrs.get(\"x-requests-remaining\")\nrl_used= hdrs.get(\"x-requests-used\")\nprint(f\"Rate limit → remaining: {rl_rem}, used: {rl_used}\")\n\n# --- flatten → lista rekordów ---\nrows: List[Dict[str, Any]] = []\nnow_iso = dt.datetime.utcnow().isoformat()\n\nfor match in data:\n    commence_iso = match.get(\"commence_time\")\n    # ISO → data (UTC)\n    event_date = None\n    if commence_iso:\n        try:\n            event_date = dt.datetime.fromisoformat(commence_iso.replace(\"Z\", \"+00:00\")).date().isoformat()\n        except Exception:\n            event_date = commence_iso[:10]\n    sport_key   = match.get(\"sport_key\")\n    sport_title = match.get(\"sport_title\")\n    event_id    = match.get(\"id\")\n    event_name  = match.get(\"home_team\") or None  # dla MMA często brak \"home/away\", ale zostawiam dla zgodności\n    teams       = match.get(\"teams\") or []\n\n    for bookmaker in match.get(\"bookmakers\", []):\n        title  = bookmaker.get(\"title\")\n        region = BOOKMAKER_REGIONS.get(title, \"unknown\")\n        last_update = bookmaker.get(\"last_update\")\n        for market in bookmaker.get(\"markets\", []):\n            if market.get(\"key\") != \"h2h\":\n                continue\n            outcomes = market.get(\"outcomes\") or []\n            if len(outcomes) != 2:\n                continue\n\n            f1 = outcomes[0].get(\"name\")\n            o1 = outcomes[0].get(\"price\")\n            f2 = outcomes[1].get(\"name\")\n            o2 = outcomes[1].get(\"price\")\n\n            rows.append({\n                \"sport_key\": sport_key,\n                \"sport_title\": sport_title,\n                \"event_id\": event_id,\n                \"event_date\": event_date,\n                \"commence_time_utc\": commence_iso,\n                \"teams\": \", \".join(teams) if teams else None,\n                \"fighter_1\": f1,\n                \"fighter_2\": f2,\n                \"odds_1\": o1,\n                \"odds_2\": o2,\n                \"bookmaker\": title,\n                \"region\": region,\n                \"bookmaker_last_update\": last_update,\n                \"fetched_at_utc\": now_iso,\n            })\n\nif not rows:\n    print(\"Brak danych do zapisania (API zwróciło pustą listę).\")\n\n# --- DataFrame + typy ---\ndf = pd.DataFrame(rows)\nif not df.empty:\n    # porządkuj typy/liczby\n    for c in [\"odds_1\",\"odds_2\"]:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    # zapis\n    df.to_csv(CSV_PATH, index=False)\n    try:\n        df.to_parquet(PQ_PATH, index=False)\n    except Exception:\n        df.to_parquet(PQ_PATH, index=False, engine=\"fastparquet\")\n\n    # podsumowanie\n    print(\"Summary:\")\n    print(\" - rows              :\", len(df))\n    print(\" - unique events     :\", df[\"event_id\"].nunique() if \"event_id\" in df else \"n/a\")\n    print(\" - unique bookmakers :\", df[\"bookmaker\"].nunique() if \"bookmaker\" in df else \"n/a\")\n    print(f\"Saved → {CSV_PATH.name}  |  {PQ_PATH.name}\")\nelse:\n    print(\"Pusta ramka — nic nie zapisano.\")\n","metadata":{"execution":{"iopub.execute_input":"2025-09-05T11:46:22.224556Z","iopub.status.busy":"2025-09-05T11:46:22.223861Z","iopub.status.idle":"2025-09-05T11:46:22.559399Z","shell.execute_reply":"2025-09-05T11:46:22.558141Z","shell.execute_reply.started":"2025-09-05T11:46:22.224525Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Rate limit → remaining: 470, used: 30\n","Summary:\n"," - rows              : 657\n"," - unique events     : 73\n"," - unique bookmakers : 26\n","Saved → UFC_betting_odds_api.csv  |  UFC_betting_odds_api.parquet\n"]}],"execution_count":11},{"cell_type":"markdown","source":"## 10) Official UFC Rankings → CSV & Parquet  \n**Purpose:** Scrape official rankings from `ufc.com/rankings` and produce a clean table suitable for joins.  \n**Inputs:** Public rankings page.  \n**Outputs:** Ranked fighters with division, rank, champion flag; saved as CSV and Parquet to `/kaggle/working`.  \n**Notes:** Light HTML parsing with BeautifulSoup; defensive handling for divisions with variable champion/contender counts.","metadata":{}},{"cell_type":"code","source":"# === UFC Rankings → CSV & Parquet (no BigQuery) ===\n# In : internet ON\n# Out: /kaggle/working/UFC_rankings.csv | .parquet\n\nimport time, datetime as dt, re\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nOUTDIR   = Path(\"/kaggle/working\"); OUTDIR.mkdir(exist_ok=True)\nBASENAME = \"UFC_rankings\"\nCSV_PATH = OUTDIR / f\"{BASENAME}.csv\"\nPQ_PATH  = OUTDIR / f\"{BASENAME}.parquet\"\n\nURL = \"https://www.ufc.com/rankings\"\nHEADERS = {\n    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                   \"Chrome/124.0.0.0 Safari/537.36\"),\n    \"Accept\": \"text/html,application/xhtml+xml\",\n}\n\ndef fetch_html(url: str, retries: int = 3, backoff: float = 1.3) -> str:\n    last_err = None\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, headers=HEADERS, timeout=20)\n            if r.status_code == 200 and r.text and len(r.text) > 1500:\n                return r.text\n            if r.status_code in (429, 500, 502, 503, 504):\n                time.sleep(backoff * (attempt + 1))\n                continue\n            r.raise_for_status()\n            return r.text\n        except Exception as e:\n            last_err = e\n            time.sleep(backoff * (attempt + 1))\n    raise RuntimeError(f\"Fetch failed after {retries} attempts: {last_err}\")\n\ndef clean_text(s: str) -> str:\n    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n\ndef parse_rankings(html: str) -> List[Dict[str, Any]]:\n    soup = BeautifulSoup(html, \"html.parser\")\n    sections = soup.find_all(\"div\", class_=\"view-grouping\")\n    out: List[Dict[str, Any]] = []\n    ranking_date = dt.datetime.utcnow().date().isoformat()\n\n    for sec in sections:\n        # Nazwa kategorii (np. \"Men's Pound-for-Pound Top Rank\", \"Flyweight\", \"Women's Strawweight\", itd.)\n        header = sec.find(\"div\", class_=\"view-grouping-header\")\n        division = clean_text(header.get_text()) if header else \"Unknown\"\n\n        # Mistrz (opcjonalnie, nie dotyczy P4P)\n        champ = sec.select_one(\"div.rankings--athlete--champion\")\n        if champ:\n            # imię/nazwisko bywają w h5 lub linku wewnątrz boxu\n            name_tag = champ.find(\"h5\") or champ.find(\"a\")\n            champion_name = clean_text(name_tag.get_text()) if name_tag else clean_text(champ.get_text())\n            if champion_name:\n                out.append({\n                    \"date\": ranking_date,\n                    \"weightclass\": division,\n                    \"fighter\": champion_name,\n                    \"rank\": 0\n                })\n\n        # Zawodnicy 1–15 (niektóre dywizje mogą mieć mniej)\n        # Stabilny selektor: komórki tytułu wiersza tabeli\n        fighter_cells = sec.select(\"td.views-field.views-field-title\")\n        if not fighter_cells:\n            # fallback: linki z nazwiskami (na wypadek zmiany klas)\n            fighter_cells = sec.select(\"table a[href*='/athlete/']\")\n        rank = 1\n        for cell in fighter_cells:\n            fighter_name = clean_text(cell.get_text())\n            if not fighter_name:\n                continue\n            out.append({\n                \"date\": ranking_date,\n                \"weightclass\": division,\n                \"fighter\": fighter_name,\n                \"rank\": rank\n            })\n            rank += 1\n\n    return out\n\n# --- run ---\nhtml = fetch_html(URL)\nrows = parse_rankings(html)\n\ndf = pd.DataFrame(rows, columns=[\"date\",\"weightclass\",\"fighter\",\"rank\"]).drop_duplicates()\ndf = df.sort_values([\"weightclass\",\"rank\",\"fighter\"]).reset_index(drop=True)\n\n# zapis\ndf.to_csv(CSV_PATH, index=False)\ntry:\n    df.to_parquet(PQ_PATH, index=False)\nexcept Exception:\n    df.to_parquet(PQ_PATH, index=False, engine=\"fastparquet\")\n\nprint(\"Summary:\")\nprint(\" - rows:\", len(df))\nprint(\" - divisions:\", df[\"weightclass\"].nunique() if not df.empty else 0)\nprint(\" - sample:\")\nprint(df.head(12))\nprint(f\"Saved → {CSV_PATH.name}  |  {PQ_PATH.name}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-09-05T11:50:22.865063Z","iopub.status.busy":"2025-09-05T11:50:22.864718Z","iopub.status.idle":"2025-09-05T11:50:23.256792Z","shell.execute_reply":"2025-09-05T11:50:23.255748Z","shell.execute_reply.started":"2025-09-05T11:50:22.865039Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary:\n"," - rows: 208\n"," - divisions: 13\n"," - sample:\n","          date   weightclass              fighter  rank\n","0   2025-09-05  Bantamweight    Merab Dvalishvili     0\n","1   2025-09-05  Bantamweight        Sean O'Malley     1\n","2   2025-09-05  Bantamweight    Umar Nurmagomedov     2\n","3   2025-09-05  Bantamweight             Petr Yan     3\n","4   2025-09-05  Bantamweight       Cory Sandhagen     4\n","5   2025-09-05  Bantamweight          Song Yadong     5\n","6   2025-09-05  Bantamweight  Deiveson Figueiredo     6\n","7   2025-09-05  Bantamweight          Marlon Vera     7\n","8   2025-09-05  Bantamweight       Mario Bautista     8\n","9   2025-09-05  Bantamweight             Rob Font     9\n","10  2025-09-05  Bantamweight       Aiemann Zahabi    10\n","11  2025-09-05  Bantamweight         Henry Cejudo    11\n","Saved → UFC_rankings.csv  |  UFC_rankings.parquet\n"]}],"execution_count":13}]}